{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCE 470 :: Information Storage and Retrieval :: Texas A&M University :: Fall 2018\n",
    "\n",
    "\n",
    "# Homework 1: Searching Amazon Reviews\n",
    "\n",
    "### 100 points [5% of your final grade]\n",
    "\n",
    "*Goals of this homework:* The objective of this homework is to understand the importance of tokenization for IR and learn how to implement a basic vector space model for document ranking.\n",
    "\n",
    "*Submission Instructions (Google Classroom):* To submit your homework, rename this notebook as  `lastname_firstinitial_hw#.ipynb`. For example, my homework submission would be: `caverlee_j_hw1.ipynb`. Submit this notebook via Google Classroom. Your IPython notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so).\n",
    "\n",
    "*Due:* September 16, 2018 by 11:59pm. For this homework, you may use up to three of your late days, meaning that no submissions will be accepted after September 19 at 11:59pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Amazon Review Data\n",
    "\n",
    "We're providing you with a small collection of 200 product (books) reviews collected from Amazon: **review.json**. You should treat each review as a unique document to be indexed by your system. You can download the reviews from Google Classroom to your local filesystem. We're going to use these reviews as the basis of an Amazon Review Search Engine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read and Parse the Amazon Data [5pts]\n",
    "\n",
    "Recall how we handled file input in Homework 0? Well, here, our goal is to read in the Amazon reviews so that we can begin to tokenize them later. For this step, you should read the dataset from your local filesystem and print the reviews. Note that our dataset is in json format. A document looks like:\n",
    "\n",
    "{'reviewerID': 'ATBGGCX75KLTT',\n",
    "  'asin': '0028610105',\n",
    "  'reviewText': 'This is the ideal book for anyone.  It has everything you can think of in it.  I was going to give it to by daughter, but I think I will keep it.  My husband is always asking how to make???  so now he can!'}\n",
    "  \n",
    "where:\n",
    "\n",
    "* **reviewerID** is a reviewer's unique ID\n",
    "* **asin** is the product's unique Amazon ID\n",
    "* **reviewText** is the actual text of the review that we care about.\n",
    "\n",
    "For this homework, you should treat the **reviewText** as a document and the **reviewerID** as the document ID.\n",
    "\n",
    "Your output should look like this:\n",
    "\n",
    "DocumentID   Document\n",
    "\n",
    "ATBGGCX75KLTT This is the ideal book for anyone.  It has everything you can think of in it.  I was going to give it to by daughter, but I think I will keep it.  My husband is always asking how to make???  so now he can!\n",
    "\n",
    "... ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "import json\n",
    "with open('review.json') as json_file:  \n",
    "    reviews = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Basic Tokenization\n",
    "\n",
    "Now that you can read the documents, let's move on to tokenization. Different tokenization methods can have a big influence on the final results. Here we try three different tokenization methods:\n",
    "\n",
    "### Strategy 1 [5pts]\n",
    "The first tokenization strategy is to tokenize each review using **whitespaces and punctuations as delimiters**. You should also lowercase all words. You should write a function to do this tokenization strategy and print the tokens. \n",
    "\n",
    "Your output should look like this:\n",
    "\n",
    "DocumentID   Tokens\n",
    "\n",
    "ATBGGCX75KLTT this is the ideal book for anyone it has everything you can think of in it ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2 [5pts]\n",
    "Another strategy is to treat a valid token as a *sequence of 3 or more characters drawn from the alphabet [a-zA-Z]*. Besides that, we also treat all whitespace and non-character data as delimiters. You should also lowercase all words which is the same as in Strategy 1.\n",
    "\n",
    "Your output should look like this:\n",
    "\n",
    "DocumentID   Tokens\n",
    "\n",
    "ATBGGCX75KLTT this the ideal book for anyone has everything you can think...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 3 [5pts]\n",
    "Further, we could also choose to apply stemming to the results from the previous strategy. For this strategy, use the Porter Stemmer. If you like, you can use the version installed in NLTK (a helpful natural language toolkit): [https://www.nltk.org/](https://www.nltk.org/)\n",
    "\n",
    "where you would do something like:\n",
    "\n",
    "```\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "```\n",
    "\n",
    "Your output should follow the style of the previous two strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations [5pts]\n",
    "Based on these three strategies, which one do you prefer? Why? You should make a clear argument with positive points and negative points. Please include examples. \n",
    "\n",
    "*Hint: There is no one correct answer here. We're mainly looking at the quality of your argument for whatever strategy you prefer.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The Most Popular Words [5pts]\n",
    "\n",
    "Great, now we can tokenize the documents. Let's take a look at the most popular words in our reviews by using Strategy 2 (same for all of the steps below). For this step, you should maintain a count of how many times each word occurs. Then you should print out the top-10 words in your reviews.\n",
    "\n",
    "Your output should look like this:\n",
    "\n",
    "Rank Token Count\n",
    "\n",
    "1 awesome 78\n",
    "2 cool 45\n",
    "... ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Zipf's Law [10pts]\n",
    "\n",
    "Recall in class our discussion of Zipf's law. Let's see if this law applies to our Amazon reviews. You should use matplotlib to plot the log-base10 term counts on the y-axis versus the log-base10 rank on the x-axis. Your aim is to create a figure like the one in Figure 5.2 of the textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "What do you observe? Is this consistent with Zipf's law?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Vector Space Model [45pts]\n",
    "\n",
    "Since the dataset is ready, we can implement our basic ranked retrieval engine now. Your goal is to implement the basic Vector Space Model using cosine and the standard TFIDF scores: that is, use log-weighted term frequency: 1+log(tf); and the log-weighted inverse document frequency: log (N/df). For the query vector use simple weights (just raw counts, no logs). Remember, you should apply the same tokenizer to your query that you also apply to your documents.\n",
    "\n",
    "You should print the top-10 Documents for an arbitrary input query. The output should like like this:\n",
    "\n",
    "Rank DocumentID  Score\n",
    "\n",
    "1     R3I9CO6P8ON1ER 0.341\n",
    "\n",
    "2     R33XAFPLMCBWRV 0.252\n",
    "\n",
    "... ...\n",
    "\n",
    "For the first example, show the results for the query: *`amazing book`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# feel free to add as many cells as you need\n",
    "# you will probably want to create several functions to help you out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now show the results for the query: *`swedish`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now show the results for the query: *`american`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now show the results for the query: *`I read it`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Finding similar reviews [15pts]\n",
    "\n",
    "For this final step, we're going to consider a query that is much longer than a few words. Indeed, we're going to use an entire review as a query. In this way, we can find similar reviews which might help us find other good reviewers or other good books to read. \n",
    "\n",
    "Suppose you were the author of the following (amazing) review:\n",
    "\n",
    "*{\"reviewerID\": \"A3SRW3A5NJY0R2\", \"asin\": \"0007444117\", \"reviewText\": \"This book was horrible. I own Divergent as a book on my Kindle Fire and I brought Insurgent from a store. I checked out Allegiant from the library because my friend raved about it at school (11th grade) and I hate it.  My real problem is the book doesn't even focuses on the problem at hand just Tris and Four's relationship. And the ending sucked...a lot. :(\"}*\n",
    "\n",
    "For this final step, use this review as your query to find similar reviews. You should output the top-10 documents as before, e.g.:\n",
    "\n",
    "Rank DocumentID  Score\n",
    "\n",
    "1     R3I9CO6P8ON1ER 0.341\n",
    "\n",
    "2     R33XAFPLMCBWRV 0.252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "What do you think ... is this a good idea? Can you figure out **why** the first review is ranked first? Do the reviews seem related? \n",
    "\n",
    "If you need some help making sense of the results, we also provide additional information about each book in the file **ProductInfo.json**. It includes the product's asin (unique ID), book title, and its description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your thoughts here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Improving Your Search Engine [5pts]\n",
    "Let's revisit our tokenization strategies. For this step, can you suggest some specific improvements to your tokenization that could improve the quality of your search engine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explanation for your approach*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
